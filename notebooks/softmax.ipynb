{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "YlFBDzu-LL6v",
      "metadata": {
        "id": "YlFBDzu-LL6v"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from torch import Tensor, nn\n",
        "from math import ceil\n",
        "from torch.autograd import Function\n",
        "\n",
        "from torch.autograd.function import Function\n",
        "import triton\n",
        "import triton.language as tl\n",
        "\n",
        "DEVICE = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
        "print(DEVICE)"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "DBNXfXgpK6gz"
      },
      "id": "DBNXfXgpK6gz",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "id": "hRyr14v0wSwN",
      "metadata": {
        "id": "hRyr14v0wSwN"
      },
      "outputs": [],
      "source": [
        "\n",
        "def validate_contiguous(x: Tensor) -> Tensor:\n",
        "    return x if x.is_contiguous() else x.contiguous()\n",
        "\n",
        "\n",
        "def validate_tensor_device(x: Tensor):\n",
        "    if not x.is_cuda:\n",
        "        message = \"Tensor must be on CUDA or TRITON_INTERPRET must be set to '1'\"\n",
        "        assert os.environ.get(\"TRITON_INTERPRET\", False) == \"1\", message\n",
        "\n",
        "\n",
        "def is_cuda_available() -> bool:\n",
        "    return torch.cuda.is_available()\n",
        "\n",
        "\n",
        "def get_device() -> torch.device:\n",
        "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "    return torch.device(device)\n",
        "\n",
        "\n",
        "\n",
        "@triton.jit\n",
        "def _softmax_triton_fwd(\n",
        "    x_pointer, y_pointer, x_stride, y_stride, n_rows, n_cols, block_size: tl.constexpr\n",
        "):\n",
        "    # get the program id: each program of the grid handles one (or more) rows of the tensor\n",
        "    pid = tl.program_id(axis=0)\n",
        "\n",
        "    # strided execution: can run the program in a strided way (e.g. for row 0, 8, 16, ...)\n",
        "    row_step = tl.num_programs(axis=0)  # n. of programs running on given axis\n",
        "\n",
        "    col_offset = tl.arange(0, block_size)\n",
        "    # Create a mask to guard memory operations against out-of-bounds accesses.\n",
        "    mask = col_offset < n_cols\n",
        "\n",
        "    # loop through the rows executed by program with this pid\n",
        "    for row_idx in tl.range(pid, n_rows, row_step):\n",
        "        x_row_pointer = x_pointer + row_idx * x_stride\n",
        "        x_col_pointer = x_row_pointer + col_offset\n",
        "\n",
        "        # compute the softmax (with shift for numerical stab.)\n",
        "        row = tl.load(x_col_pointer, mask, other=-float(\"inf\"))\n",
        "\n",
        "        row_minus_max = row - tl.max(row, axis=0)\n",
        "        num = tl.exp(row_minus_max)\n",
        "        den = tl.sum(num, axis=0)\n",
        "        y = num / den\n",
        "\n",
        "        y_row_pointer = y_pointer + row_idx * y_stride\n",
        "        y_col_pointer = y_row_pointer + col_offset\n",
        "        tl.store(y_col_pointer, y, mask)\n",
        "\n",
        "\n",
        "def _softmax_fwd(x: Tensor, block_size: int = 1024) -> Tensor:\n",
        "    validate_tensor_device(x)\n",
        "\n",
        "    n_rows, n_cols = x.shape\n",
        "    y = torch.empty_like(x)\n",
        "    grid = (n_rows,)\n",
        "    block_size = triton.next_power_of_2(n_cols)  # Used to tile the row\n",
        "\n",
        "    _softmax_triton_fwd[grid](\n",
        "        x, y, x.stride(0), y.stride(0), n_rows, n_cols, block_size\n",
        "    )\n",
        "    return y\n",
        "\n",
        "\n",
        "@triton.jit\n",
        "def load_tensor_row(x_ptr, row_idx, x_stride, col_ptr_offset, mask, other):\n",
        "    # row pointer\n",
        "    row_ptr = x_ptr + row_idx * x_stride\n",
        "\n",
        "    # col pointer\n",
        "    col_ptr = row_ptr + col_ptr_offset\n",
        "\n",
        "    # load all tensors\n",
        "    return tl.load(col_ptr, mask, other=other)\n",
        "\n",
        "\n",
        "@triton.jit\n",
        "def _softmax_triton_bwd(\n",
        "    s_ptr,\n",
        "    grad_output_ptr,\n",
        "    grad_input_ptr,\n",
        "    s_stride,\n",
        "    grad_output_stride,\n",
        "    grad_input_stride,\n",
        "    nrows,\n",
        "    ncols,\n",
        "    block_size: tl.constexpr,\n",
        "):\n",
        "\n",
        "    pid = tl.program_id(axis=0)\n",
        "    row_step = tl.num_programs(axis=0)\n",
        "    col_ptr_offset = tl.arange(0, block_size)\n",
        "    mask = col_ptr_offset < ncols\n",
        "\n",
        "    for row_idx in tl.range(pid, nrows, row_step):\n",
        "        ### load s\n",
        "        s = tl.load(s_ptr + row_idx * s_stride + col_ptr_offset, mask)\n",
        "\n",
        "        ### load grad_output\n",
        "        grad_output = tl.load(\n",
        "            grad_output_ptr + row_idx * grad_output_stride + col_ptr_offset,\n",
        "            mask,\n",
        "        )\n",
        "\n",
        "        # perform coputations\n",
        "        # grad_input = s * grad_output\n",
        "        alpha = tl.sum(s * grad_output, axis=0)\n",
        "        grad_input = s * (grad_output - alpha)\n",
        "\n",
        "        # store the results\n",
        "        tl.store(\n",
        "            grad_input_ptr + row_idx * grad_input_stride + col_ptr_offset,\n",
        "            grad_input,\n",
        "            mask,\n",
        "        )\n",
        "\n",
        "\n",
        "def _softmax_bwd(s: Tensor, grad_output: Tensor, block_size: int = 1024) -> Tensor:\n",
        "    for x in (s, grad_output):\n",
        "        validate_tensor_device(x)\n",
        "\n",
        "    nrows, ncols = s.shape\n",
        "    grad_input = torch.empty_like(grad_output).to(s.device)\n",
        "    grid = (nrows,)\n",
        "\n",
        "    block_size = triton.next_power_of_2(ncols)\n",
        "\n",
        "    _softmax_triton_bwd[grid](\n",
        "        s,\n",
        "        grad_output,\n",
        "        grad_input,\n",
        "        s.stride(0),\n",
        "        grad_output.stride(0),\n",
        "        grad_input.stride(0),\n",
        "        nrows,\n",
        "        ncols,\n",
        "        block_size,\n",
        "    )\n",
        "\n",
        "    return grad_input\n",
        "\n",
        "\n",
        "class SoftmaxFunction(Function):\n",
        "    def forward(ctx, x):\n",
        "        s = _softmax_fwd(x)\n",
        "        ctx.save_for_backward(s)\n",
        "        return s\n",
        "\n",
        "    def backward(ctx, grad_output):\n",
        "        (s,) = ctx.saved_tensors\n",
        "\n",
        "        # exp = torch.exp(x)\n",
        "        # den = exp.sum(dim=1)\n",
        "        # derivative = (exp * (den - exp)) / den ** 2\n",
        "\n",
        "        # y = torch.softmax(x, dim=1)\n",
        "        # alpha = (s * grad_output).sum(dim=1, keepdim=True)\n",
        "        # grad_input = s * (grad_output - alpha)\n",
        "\n",
        "        grad_input = _softmax_bwd(s, grad_output)\n",
        "\n",
        "        # derivative = _softmax_bwd(x)\n",
        "        return grad_input\n",
        "\n",
        "\n",
        "class Softmax(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "\n",
        "    def forward(self, x: Tensor) -> Tensor:\n",
        "        if x.ndim == 1:\n",
        "            return SoftmaxFunction.apply(x[None, :]).squeeze()\n",
        "        if x.ndim > 2:\n",
        "            x = validate_contiguous(x)\n",
        "            return SoftmaxFunction.apply(x.view(-1, x.shape[-1])).view(x.shape)\n",
        "        return SoftmaxFunction.apply(x)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "x = torch.rand(10, 100).to(DEVICE)\n",
        "Softmax()(x)"
      ],
      "metadata": {
        "id": "zoLdas5jLobU",
        "outputId": "7be0326a-2777-4ef2-a4d8-5a244a76e916",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "zoLdas5jLobU",
      "execution_count": 11,
      "outputs": [
        {
          "data": {
            "text/plain": [
              "tensor([[0.0133, 0.0090, 0.0086, 0.0125, 0.0082, 0.0080, 0.0063, 0.0111, 0.0067,\n",
              "         0.0115, 0.0096, 0.0104, 0.0097, 0.0074, 0.0139, 0.0098, 0.0075, 0.0090,\n",
              "         0.0066, 0.0076, 0.0108, 0.0064, 0.0084, 0.0125, 0.0080, 0.0138, 0.0085,\n",
              "         0.0068, 0.0155, 0.0151, 0.0118, 0.0103, 0.0081, 0.0135, 0.0083, 0.0068,\n",
              "         0.0096, 0.0112, 0.0063, 0.0097, 0.0100, 0.0078, 0.0094, 0.0130, 0.0081,\n",
              "         0.0084, 0.0123, 0.0082, 0.0094, 0.0080, 0.0069, 0.0082, 0.0094, 0.0153,\n",
              "         0.0085, 0.0083, 0.0063, 0.0088, 0.0159, 0.0134, 0.0074, 0.0166, 0.0092,\n",
              "         0.0111, 0.0088, 0.0081, 0.0062, 0.0126, 0.0077, 0.0107, 0.0113, 0.0083,\n",
              "         0.0160, 0.0083, 0.0077, 0.0133, 0.0093, 0.0137, 0.0098, 0.0123, 0.0067,\n",
              "         0.0066, 0.0112, 0.0069, 0.0075, 0.0125, 0.0134, 0.0121, 0.0165, 0.0103,\n",
              "         0.0109, 0.0091, 0.0124, 0.0112, 0.0084, 0.0100, 0.0154, 0.0081, 0.0081,\n",
              "         0.0110],\n",
              "        [0.0082, 0.0119, 0.0119, 0.0084, 0.0075, 0.0152, 0.0067, 0.0162, 0.0075,\n",
              "         0.0089, 0.0152, 0.0109, 0.0144, 0.0119, 0.0068, 0.0080, 0.0063, 0.0142,\n",
              "         0.0124, 0.0099, 0.0119, 0.0148, 0.0065, 0.0164, 0.0121, 0.0166, 0.0087,\n",
              "         0.0079, 0.0082, 0.0063, 0.0066, 0.0079, 0.0072, 0.0075, 0.0076, 0.0073,\n",
              "         0.0093, 0.0137, 0.0143, 0.0092, 0.0066, 0.0100, 0.0111, 0.0119, 0.0072,\n",
              "         0.0159, 0.0079, 0.0065, 0.0063, 0.0071, 0.0067, 0.0155, 0.0064, 0.0089,\n",
              "         0.0072, 0.0083, 0.0089, 0.0079, 0.0093, 0.0075, 0.0065, 0.0067, 0.0152,\n",
              "         0.0068, 0.0109, 0.0104, 0.0093, 0.0079, 0.0161, 0.0142, 0.0121, 0.0119,\n",
              "         0.0072, 0.0102, 0.0129, 0.0086, 0.0090, 0.0095, 0.0068, 0.0144, 0.0065,\n",
              "         0.0115, 0.0069, 0.0081, 0.0119, 0.0166, 0.0166, 0.0152, 0.0066, 0.0077,\n",
              "         0.0104, 0.0088, 0.0122, 0.0099, 0.0143, 0.0097, 0.0083, 0.0082, 0.0078,\n",
              "         0.0067],\n",
              "        [0.0115, 0.0087, 0.0081, 0.0151, 0.0097, 0.0079, 0.0099, 0.0065, 0.0063,\n",
              "         0.0095, 0.0069, 0.0138, 0.0084, 0.0065, 0.0142, 0.0125, 0.0118, 0.0077,\n",
              "         0.0144, 0.0074, 0.0095, 0.0108, 0.0134, 0.0067, 0.0149, 0.0133, 0.0150,\n",
              "         0.0080, 0.0106, 0.0119, 0.0069, 0.0082, 0.0076, 0.0129, 0.0143, 0.0066,\n",
              "         0.0101, 0.0078, 0.0072, 0.0130, 0.0062, 0.0131, 0.0100, 0.0151, 0.0096,\n",
              "         0.0102, 0.0150, 0.0062, 0.0066, 0.0131, 0.0103, 0.0134, 0.0068, 0.0120,\n",
              "         0.0148, 0.0138, 0.0073, 0.0060, 0.0150, 0.0076, 0.0110, 0.0151, 0.0064,\n",
              "         0.0077, 0.0076, 0.0058, 0.0099, 0.0146, 0.0091, 0.0069, 0.0118, 0.0064,\n",
              "         0.0117, 0.0079, 0.0126, 0.0078, 0.0063, 0.0151, 0.0093, 0.0085, 0.0129,\n",
              "         0.0063, 0.0077, 0.0080, 0.0061, 0.0059, 0.0073, 0.0132, 0.0074, 0.0064,\n",
              "         0.0154, 0.0120, 0.0066, 0.0091, 0.0149, 0.0154, 0.0066, 0.0076, 0.0094,\n",
              "         0.0122],\n",
              "        [0.0134, 0.0088, 0.0090, 0.0103, 0.0069, 0.0105, 0.0103, 0.0120, 0.0125,\n",
              "         0.0104, 0.0129, 0.0061, 0.0078, 0.0124, 0.0091, 0.0059, 0.0152, 0.0079,\n",
              "         0.0128, 0.0142, 0.0083, 0.0144, 0.0070, 0.0075, 0.0108, 0.0138, 0.0066,\n",
              "         0.0085, 0.0113, 0.0080, 0.0142, 0.0062, 0.0144, 0.0084, 0.0150, 0.0079,\n",
              "         0.0094, 0.0137, 0.0122, 0.0139, 0.0133, 0.0093, 0.0153, 0.0086, 0.0115,\n",
              "         0.0122, 0.0097, 0.0132, 0.0088, 0.0109, 0.0082, 0.0148, 0.0075, 0.0102,\n",
              "         0.0066, 0.0108, 0.0083, 0.0140, 0.0089, 0.0088, 0.0061, 0.0133, 0.0137,\n",
              "         0.0084, 0.0126, 0.0090, 0.0120, 0.0093, 0.0071, 0.0087, 0.0115, 0.0123,\n",
              "         0.0076, 0.0059, 0.0099, 0.0080, 0.0092, 0.0125, 0.0068, 0.0088, 0.0087,\n",
              "         0.0131, 0.0066, 0.0058, 0.0095, 0.0088, 0.0063, 0.0086, 0.0083, 0.0064,\n",
              "         0.0110, 0.0073, 0.0084, 0.0087, 0.0107, 0.0102, 0.0145, 0.0120, 0.0057,\n",
              "         0.0062],\n",
              "        [0.0159, 0.0121, 0.0146, 0.0084, 0.0072, 0.0142, 0.0159, 0.0131, 0.0068,\n",
              "         0.0070, 0.0131, 0.0119, 0.0084, 0.0149, 0.0117, 0.0064, 0.0070, 0.0106,\n",
              "         0.0135, 0.0136, 0.0109, 0.0071, 0.0095, 0.0122, 0.0129, 0.0098, 0.0122,\n",
              "         0.0122, 0.0093, 0.0091, 0.0110, 0.0081, 0.0100, 0.0153, 0.0068, 0.0066,\n",
              "         0.0081, 0.0083, 0.0063, 0.0157, 0.0082, 0.0079, 0.0066, 0.0102, 0.0059,\n",
              "         0.0071, 0.0133, 0.0141, 0.0133, 0.0121, 0.0152, 0.0154, 0.0067, 0.0102,\n",
              "         0.0087, 0.0088, 0.0145, 0.0075, 0.0138, 0.0093, 0.0098, 0.0131, 0.0073,\n",
              "         0.0157, 0.0061, 0.0073, 0.0065, 0.0102, 0.0072, 0.0082, 0.0061, 0.0149,\n",
              "         0.0078, 0.0094, 0.0103, 0.0086, 0.0073, 0.0105, 0.0077, 0.0150, 0.0086,\n",
              "         0.0079, 0.0059, 0.0116, 0.0072, 0.0079, 0.0095, 0.0096, 0.0107, 0.0079,\n",
              "         0.0100, 0.0062, 0.0158, 0.0150, 0.0061, 0.0063, 0.0063, 0.0080, 0.0059,\n",
              "         0.0074],\n",
              "        [0.0145, 0.0119, 0.0128, 0.0073, 0.0100, 0.0073, 0.0074, 0.0115, 0.0161,\n",
              "         0.0078, 0.0126, 0.0116, 0.0098, 0.0158, 0.0146, 0.0080, 0.0138, 0.0073,\n",
              "         0.0082, 0.0146, 0.0061, 0.0088, 0.0081, 0.0153, 0.0099, 0.0069, 0.0068,\n",
              "         0.0114, 0.0087, 0.0122, 0.0145, 0.0065, 0.0094, 0.0109, 0.0092, 0.0150,\n",
              "         0.0070, 0.0080, 0.0077, 0.0160, 0.0119, 0.0135, 0.0073, 0.0080, 0.0102,\n",
              "         0.0065, 0.0105, 0.0121, 0.0072, 0.0158, 0.0079, 0.0158, 0.0081, 0.0139,\n",
              "         0.0101, 0.0102, 0.0098, 0.0074, 0.0112, 0.0094, 0.0080, 0.0069, 0.0097,\n",
              "         0.0127, 0.0095, 0.0065, 0.0062, 0.0070, 0.0134, 0.0062, 0.0082, 0.0132,\n",
              "         0.0141, 0.0066, 0.0081, 0.0095, 0.0117, 0.0116, 0.0095, 0.0130, 0.0148,\n",
              "         0.0066, 0.0110, 0.0118, 0.0093, 0.0072, 0.0143, 0.0066, 0.0109, 0.0079,\n",
              "         0.0082, 0.0073, 0.0082, 0.0080, 0.0107, 0.0062, 0.0075, 0.0077, 0.0070,\n",
              "         0.0090],\n",
              "        [0.0152, 0.0079, 0.0140, 0.0111, 0.0080, 0.0137, 0.0135, 0.0118, 0.0070,\n",
              "         0.0093, 0.0067, 0.0144, 0.0102, 0.0084, 0.0106, 0.0146, 0.0089, 0.0147,\n",
              "         0.0101, 0.0123, 0.0132, 0.0099, 0.0078, 0.0071, 0.0065, 0.0063, 0.0080,\n",
              "         0.0061, 0.0095, 0.0099, 0.0059, 0.0153, 0.0066, 0.0057, 0.0060, 0.0148,\n",
              "         0.0096, 0.0152, 0.0071, 0.0094, 0.0073, 0.0108, 0.0115, 0.0093, 0.0065,\n",
              "         0.0101, 0.0125, 0.0098, 0.0132, 0.0095, 0.0070, 0.0059, 0.0091, 0.0101,\n",
              "         0.0119, 0.0059, 0.0093, 0.0057, 0.0100, 0.0152, 0.0081, 0.0152, 0.0102,\n",
              "         0.0101, 0.0091, 0.0093, 0.0059, 0.0064, 0.0146, 0.0062, 0.0089, 0.0083,\n",
              "         0.0088, 0.0106, 0.0139, 0.0094, 0.0072, 0.0126, 0.0074, 0.0078, 0.0076,\n",
              "         0.0119, 0.0148, 0.0104, 0.0117, 0.0090, 0.0074, 0.0085, 0.0121, 0.0150,\n",
              "         0.0110, 0.0112, 0.0107, 0.0081, 0.0070, 0.0129, 0.0150, 0.0144, 0.0064,\n",
              "         0.0122],\n",
              "        [0.0074, 0.0122, 0.0101, 0.0082, 0.0058, 0.0071, 0.0134, 0.0120, 0.0137,\n",
              "         0.0079, 0.0067, 0.0081, 0.0091, 0.0079, 0.0105, 0.0083, 0.0145, 0.0095,\n",
              "         0.0099, 0.0156, 0.0074, 0.0067, 0.0067, 0.0072, 0.0108, 0.0128, 0.0134,\n",
              "         0.0085, 0.0105, 0.0102, 0.0156, 0.0112, 0.0108, 0.0118, 0.0093, 0.0149,\n",
              "         0.0075, 0.0090, 0.0108, 0.0093, 0.0109, 0.0100, 0.0074, 0.0128, 0.0073,\n",
              "         0.0064, 0.0093, 0.0154, 0.0130, 0.0068, 0.0135, 0.0075, 0.0072, 0.0093,\n",
              "         0.0150, 0.0067, 0.0120, 0.0080, 0.0104, 0.0117, 0.0060, 0.0077, 0.0131,\n",
              "         0.0083, 0.0078, 0.0107, 0.0087, 0.0081, 0.0127, 0.0058, 0.0067, 0.0099,\n",
              "         0.0127, 0.0113, 0.0088, 0.0077, 0.0112, 0.0078, 0.0108, 0.0145, 0.0136,\n",
              "         0.0137, 0.0108, 0.0139, 0.0078, 0.0117, 0.0114, 0.0073, 0.0081, 0.0092,\n",
              "         0.0077, 0.0071, 0.0075, 0.0142, 0.0127, 0.0080, 0.0097, 0.0092, 0.0111,\n",
              "         0.0124],\n",
              "        [0.0113, 0.0094, 0.0073, 0.0063, 0.0096, 0.0069, 0.0090, 0.0086, 0.0078,\n",
              "         0.0125, 0.0063, 0.0117, 0.0074, 0.0137, 0.0133, 0.0111, 0.0150, 0.0071,\n",
              "         0.0115, 0.0122, 0.0079, 0.0113, 0.0139, 0.0106, 0.0076, 0.0121, 0.0087,\n",
              "         0.0077, 0.0080, 0.0113, 0.0065, 0.0137, 0.0104, 0.0133, 0.0140, 0.0097,\n",
              "         0.0116, 0.0124, 0.0076, 0.0090, 0.0075, 0.0080, 0.0081, 0.0111, 0.0056,\n",
              "         0.0117, 0.0089, 0.0066, 0.0120, 0.0116, 0.0119, 0.0077, 0.0068, 0.0091,\n",
              "         0.0067, 0.0102, 0.0060, 0.0143, 0.0067, 0.0107, 0.0118, 0.0145, 0.0083,\n",
              "         0.0103, 0.0078, 0.0113, 0.0140, 0.0129, 0.0057, 0.0124, 0.0104, 0.0077,\n",
              "         0.0112, 0.0123, 0.0128, 0.0078, 0.0126, 0.0068, 0.0061, 0.0097, 0.0070,\n",
              "         0.0142, 0.0092, 0.0102, 0.0086, 0.0134, 0.0096, 0.0140, 0.0065, 0.0105,\n",
              "         0.0144, 0.0063, 0.0084, 0.0092, 0.0139, 0.0123, 0.0083, 0.0144, 0.0058,\n",
              "         0.0113],\n",
              "        [0.0106, 0.0072, 0.0127, 0.0080, 0.0104, 0.0095, 0.0099, 0.0103, 0.0138,\n",
              "         0.0107, 0.0129, 0.0098, 0.0068, 0.0150, 0.0061, 0.0129, 0.0112, 0.0084,\n",
              "         0.0121, 0.0078, 0.0079, 0.0072, 0.0127, 0.0129, 0.0085, 0.0105, 0.0060,\n",
              "         0.0138, 0.0100, 0.0065, 0.0080, 0.0101, 0.0083, 0.0067, 0.0139, 0.0126,\n",
              "         0.0144, 0.0069, 0.0120, 0.0065, 0.0155, 0.0066, 0.0078, 0.0077, 0.0125,\n",
              "         0.0102, 0.0116, 0.0108, 0.0072, 0.0071, 0.0115, 0.0134, 0.0066, 0.0120,\n",
              "         0.0154, 0.0161, 0.0114, 0.0091, 0.0100, 0.0085, 0.0065, 0.0066, 0.0089,\n",
              "         0.0066, 0.0064, 0.0078, 0.0115, 0.0125, 0.0111, 0.0075, 0.0079, 0.0071,\n",
              "         0.0063, 0.0141, 0.0129, 0.0109, 0.0144, 0.0122, 0.0099, 0.0063, 0.0066,\n",
              "         0.0123, 0.0063, 0.0144, 0.0072, 0.0078, 0.0135, 0.0071, 0.0071, 0.0108,\n",
              "         0.0080, 0.0122, 0.0145, 0.0092, 0.0090, 0.0122, 0.0061, 0.0077, 0.0133,\n",
              "         0.0149]], device='cuda:0')"
            ]
          },
          "execution_count": 11,
          "metadata": {},
          "output_type": "execute_result"
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "# def fwd(x, provider):\n",
        "#     if provider == \"torch\":\n",
        "#         return torch.nn.functional.softmax(x, dim=-1)#(x, axis=-1)\n",
        "#     elif provider == 'triton':\n",
        "#         return _fwd(x)\n",
        "#     else:\n",
        "#         raise ValueError\n",
        "\n",
        "# def bwd(x, provider):\n",
        "#     x.requires_grad = True\n",
        "#     out = fwd(x, provider)\n",
        "\n",
        "#     loss = out.sum()\n",
        "#     loss.backward()\n",
        "\n",
        "def measure_memory(f, *args, **kwargs):\n",
        "    torch.cuda.synchronize()\n",
        "    torch.cuda.reset_peak_memory_stats()\n",
        "    torch.cuda.empty_cache()\n",
        "\n",
        "    f(*args, **kwargs)  # run your function once\n",
        "\n",
        "    torch.cuda.synchronize()\n",
        "    peak = torch.cuda.max_memory_allocated() / 1e6  # MB\n",
        "    return peak\n",
        "\n",
        "\n",
        "base_benchmark_kwargs = {\n",
        "        \"x_names\":['N'],  # argument names to use as an x-axis for the plot\n",
        "        \"x_vals\":[128 * i for i in range(2, 50, 10)],  # different possible values for `x_name`\n",
        "        \"line_arg\":'provider',  # argument name whose value corresponds to a different line in the plot\n",
        "        \"line_vals\":['triton', 'torch'],  # possible values for `line_arg``\n",
        "        \"line_names\":[\"Triton\", \"Torch\"],  # label name for the lines\n",
        "        \"styles\":[('blue', '-'), ('green', '-')],  # line styles\n",
        "        \"ylabel\":\"GB/s\",  # label name for the y-axis\n",
        "        \"plot_name\":\"softmax\",  # name for the plot. Used also as a file name for saving the plot.\n",
        "        \"args\":{'M': 4096} # values for function arguments not in `x_names` and `y_name`\n",
        "}\n",
        "\n",
        "from copy import deepcopy\n",
        "configs = []\n",
        "for bench_kind in ['timing', 'memory']:\n",
        "    for mode in ['fwd', 'bwd']:\n",
        "        _kwargs = deepcopy(base_benchmark_kwargs)\n",
        "        _kwargs['args'].update({\"mode\" : mode})\n",
        "        _kwargs['args'].update({\"bench_kind\" : bench_kind})\n",
        "        _kwargs['plot_name'] += f' - {bench_kind} - {mode}'\n",
        "        configs.append(triton.testing.Benchmark(**_kwargs))\n",
        "\n",
        "\n",
        "_fwd = Softmax()\n",
        "\n",
        "\n",
        "def fwd(x, provider):\n",
        "    if provider == \"torch\":\n",
        "        return torch.nn.functional.softmax(x, dim=-1)#(x, axis=-1)\n",
        "    elif provider == 'triton':\n",
        "        return _fwd(x)\n",
        "    else:\n",
        "        raise ValueError\n",
        "\n",
        "def bwd(x, provider):\n",
        "    x.requires_grad = True\n",
        "    out = benchmark_fwd(x, provider)\n",
        "    loss = out.sum()\n",
        "    loss.backward()\n",
        "\n",
        "MAP_FWD_BKW = {\n",
        "    \"fwd\" : fwd,\n",
        "    \"bwd\" : bwd,\n",
        "}\n",
        "\n",
        "def measure_memory(f, *args, **kwargs):\n",
        "    torch.cuda.synchronize()\n",
        "    torch.cuda.reset_peak_memory_stats()\n",
        "    torch.cuda.empty_cache()\n",
        "\n",
        "    f(*args, **kwargs)  # run your function once\n",
        "\n",
        "    torch.cuda.synchronize()\n",
        "    peak = torch.cuda.max_memory_allocated() / 1e6  # MB\n",
        "    return peak\n",
        "\n",
        "def benchmark_kernel():\n",
        "\n",
        "    for c in configs:\n",
        "        print(c)\n",
        "\n",
        "    # @triton.testing.perf_report(configs)\n",
        "    # def benchmark(M, N, provider, mode, bench_kind):\n",
        "    #     x = torch.randn(M, N, device=DEVICE, dtype=torch.bfloat16)\n",
        "    #     stream = getattr(torch, DEVICE.type).Stream()\n",
        "    #     getattr(torch, DEVICE.type).set_stream(stream)\n",
        "    #     if bench_kind == \"timing\":\n",
        "    #         ms = triton.testing.do_bench(lambda: MAP_FWD_BKW[mode](x, provider))\n",
        "    #         gbps = lambda ms: 2 * x.numel() * x.element_size() * 1e-9 / (ms * 1e-3)\n",
        "    #         return gbps(ms)\n",
        "    #     elif bench_kind == \"memory\":\n",
        "    #         mem_mb = measure_memory(MAP_FWD_BKW[mode], x, provider)\n",
        "    #         return mem_mb\n",
        "    #     raise ValueError(f\"bench_kind must be either 'timing' or 'memory', got {bench_kind}\")\n",
        "\n",
        "    # benchmark.run(show_plots=True, print_data=True)\n",
        "\n",
        "benchmark_kernel()"
      ],
      "metadata": {
        "id": "BAHRvJm8KN8B"
      },
      "id": "BAHRvJm8KN8B",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "GIpa9-ZtKzIl"
      },
      "id": "GIpa9-ZtKzIl",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "UEgA-Ach9A3j",
      "metadata": {
        "id": "UEgA-Ach9A3j"
      },
      "outputs": [],
      "source": [
        "from torch import nn, Tensor\n",
        "import torch\n",
        "from torch.autograd.function import Function\n",
        "import triton\n",
        "import triton.language as tl\n",
        "from math import ceil, pi\n",
        "\n",
        "import math\n",
        "\n",
        "torch_ref_module = nn.GELU\n",
        "\n",
        "\n",
        "# @triton.jit\n",
        "# def _gelu_fwd_triton(\n",
        "#     x_ptr, tanh_ptr, phi_ptr, act_ptr, num_elements, block_size: tl.constexpr\n",
        "# ):\n",
        "\n",
        "#     pid = tl.program_id(axis=0)\n",
        "#     ptr_offset = pid * block_size + tl.arange(0, block_size)\n",
        "\n",
        "#     mask = ptr_offset < num_elements\n",
        "\n",
        "#     x = tl.load(x_ptr + ptr_offset, mask)\n",
        "\n",
        "#     angle = tl.sqrt(2 / math.pi) * (x + 0.044715 * x * x * x)\n",
        "#     tanh = (tl.exp(2 * angle) - 1) / (tl.exp(2 * angle) + 1)\n",
        "#     phi = 0.5 * (1 + tanh)\n",
        "#     chunk_act = x * phi\n",
        "\n",
        "#     tl.store(tanh_ptr + ptr_offset, tanh, mask)\n",
        "#     tl.store(phi_ptr + ptr_offset, phi, mask)\n",
        "#     tl.store(act_ptr + ptr_offset, chunk_act, mask)\n",
        "\n",
        "\n",
        "# def _gelu_fwd_triton(x: Tensor, block_size: int = 2048) -> Tensor:\n",
        "#     validate_tensor_device(x)\n",
        "\n",
        "#     num_elements = x.numel()\n",
        "#     grid = (ceil(num_elements / block_size),)\n",
        "#     act = torch.empty_like(x).to(x.device)\n",
        "#     phi = torch.empty_like(x).to(x.device)\n",
        "#     tanh = torch.empty_like(x).to(x.device)\n",
        "\n",
        "#     _gelu_fwd_triton[grid](x, tanh, phi, act, num_elements, block_size)\n",
        "#     return act, tanh, phi\n",
        "\n",
        "@triton.jit\n",
        "def gelu_kernel(x_pointer, out_pointer, num_elements, block_size: tl.constexpr):\n",
        "\n",
        "    pid = tl.program_id(axis=0)\n",
        "    pointer_offset = pid*block_size + tl.arange(0, block_size)\n",
        "\n",
        "    mask = pointer_offset < num_elements\n",
        "\n",
        "    x = tl.load(x_pointer + pointer_offset, mask)\n",
        "\n",
        "    const = tl.sqrt(2 / math.pi)\n",
        "    angle = const * (x + 0.044715 * x * x * x)\n",
        "    tmp = tl.exp(2 * angle)\n",
        "    tanh = (tmp - 1) / (tmp + 1)\n",
        "    chunk_res = 0.5 * x * (1 + tanh)\n",
        "\n",
        "    tl.store(out_pointer + pointer_offset, chunk_res, mask)\n",
        "\n",
        "\n",
        "def _gelu_fwd(x:Tensor, block_size: int = 2048) -> Tensor:\n",
        "\n",
        "    num_elements = x.numel()\n",
        "    grid = ceil(num_elements/block_size),\n",
        "\n",
        "    out = torch.empty_like(x).to(x.device)\n",
        "\n",
        "    gelu_kernel[grid](x, out, num_elements, block_size)\n",
        "\n",
        "    return out\n",
        "\n",
        "\n",
        "@triton.jit\n",
        "def _gelu_bwd_triton(\n",
        "    x_ptr, tanh_ptr, phi_ptr, derivative_ptr, num_elements, block_size: tl.constexpr\n",
        "):\n",
        "\n",
        "    pid = tl.program_id(axis=0)\n",
        "    ptr_offset = pid * block_size + tl.arange(0, block_size)\n",
        "    mask = ptr_offset < num_elements\n",
        "\n",
        "    x = tl.load(x_ptr + ptr_offset, mask)\n",
        "    tanh = tl.load(tanh_ptr + ptr_offset, mask)\n",
        "    phi = tl.load(phi_ptr + ptr_offset, mask)\n",
        "\n",
        "    factor = 1 / tl.sqrt(2 * math.pi)\n",
        "    # angle = tl.sqrt(2 / math.pi) * (x + 0.044715 * x ** 3)\n",
        "    # tanh = nn.functional.tanh(gx)\n",
        "    phi_prime = factor * (1 - tanh * tanh) * (1 + 3 * 0.044715 * x * x)\n",
        "\n",
        "    # phi = 0.5 * (1 + tanh)\n",
        "\n",
        "    derivative = x * phi_prime + phi\n",
        "\n",
        "    tl.store(derivative_ptr + ptr_offset, derivative, mask)\n",
        "\n",
        "\n",
        "def _gelu_bwd(x: Tensor, tanh: Tensor, phi: Tensor, block_size: int = 2048) -> Tensor:\n",
        "    for t in [x, tanh, phi]:\n",
        "        validate_tensor_device(t)\n",
        "\n",
        "    num_elements = x.numel()\n",
        "    grid = (ceil(num_elements / block_size),)\n",
        "\n",
        "    derivative = torch.empty_like(x).to(x.device)\n",
        "\n",
        "    _gelu_bwd_triton[grid](x, tanh, phi, derivative, num_elements, block_size)\n",
        "\n",
        "    return derivative\n",
        "\n",
        "\n",
        "class GeluFunction(Function):\n",
        "    @staticmethod\n",
        "    def forward(ctx, x: Tensor):\n",
        "        act = _gelu_fwd(x)\n",
        "        ctx.save_for_backward(x) #, tanh, phi)\n",
        "        # return _gelu(x)\n",
        "        return act\n",
        "\n",
        "    @staticmethod\n",
        "    def backward(ctx, grad_output):\n",
        "        x, tanh, phi = ctx.saved_tensors\n",
        "\n",
        "        # factor = 1 / (2 * math.pi) ** 0.5\n",
        "        # gx = (2 / math.pi) ** 0.5 * (x + 0.044715 * x ** 3)\n",
        "        # tanh = nn.functional.tanh(gx)\n",
        "        # phi_prime = factor * (1 - tanh**2) * (1 + 3 * 0.044715 * x ** 2)\n",
        "\n",
        "        # phi = 0.5 * (1 + tanh)\n",
        "\n",
        "        # derivative = phi + x * phi_prime\n",
        "        derivative = _gelu_bwd(x, tanh, phi)\n",
        "        return derivative * grad_output\n",
        "\n",
        "\n",
        "class GELU(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "\n",
        "    def forward(self, x: Tensor) -> Tensor:\n",
        "        if x.ndim > 1:\n",
        "            x = validate_contiguous(x)\n",
        "            return GeluFunction.apply(x.view(-1)).view(x.shape)\n",
        "        return GeluFunction.apply(x)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "96fa71ce",
      "metadata": {
        "id": "96fa71ce"
      },
      "outputs": [],
      "source": [
        "import psutil\n",
        "from time import time\n",
        "import os\n",
        "from typing import Optional\n",
        "\n",
        "class Profiler:\n",
        "    peak_alloc : Optional[float] = None\n",
        "\n",
        "    def __init__(self, device=None):\n",
        "        self.device = torch.device(device or \"cuda\")\n",
        "\n",
        "    def __enter__(self):\n",
        "        self.t_start = time()\n",
        "        if torch.cuda.is_available():\n",
        "            torch.cuda.synchronize(self.device)\n",
        "            torch.cuda.reset_peak_memory_stats(self.device)\n",
        "            self.start_alloc = torch.cuda.memory_allocated(self.device)\n",
        "            self.start_reserved = torch.cuda.memory_reserved(self.device)\n",
        "            self.start_rss = (\n",
        "                psutil.Process(os.getpid()).memory_info().rss if psutil else None\n",
        "            )\n",
        "        return self\n",
        "\n",
        "    def __exit__(self, exc_type, exc, tb):\n",
        "        self.t_elapsed = time() - self.t_start\n",
        "        if torch.cuda.is_available():\n",
        "            torch.cuda.synchronize(self.device)\n",
        "            self.end_alloc = torch.cuda.memory_allocated(self.device)\n",
        "            self.end_reserved = torch.cuda.memory_reserved(self.device)\n",
        "            self.peak_alloc = torch.cuda.max_memory_allocated(self.device)\n",
        "            self.peak_reserved = torch.cuda.max_memory_reserved(self.device)\n",
        "\n",
        "    # def report(self, unit=1024**2):\n",
        "    #     to_mb = lambda b: None if b is None else b / unit\n",
        "    #     return {\n",
        "    #         \"alloc_delta_MB\": to_mb(self.end_alloc - self.start_alloc),\n",
        "    #         \"reserved_delta_MB\": to_mb(self.end_reserved - self.start_reserved),\n",
        "    #         \"peak_alloc_MB\": to_mb(self.peak_alloc),\n",
        "    #         \"peak_reserved_MB\": to_mb(self.peak_reserved),\n",
        "    #         \"cpu_rss_MB\": to_mb(self.start_rss) if self.start_rss is not None else None,\n",
        "    #     }"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "35dba6cf",
      "metadata": {
        "id": "35dba6cf"
      },
      "outputs": [],
      "source": [
        "from tqdm import tqdm\n",
        "\n",
        "Ns=[128 * i for i in range(2, 50, 5)]\n",
        "M = 4096\n",
        "\n",
        "summary = {\n",
        "    \"time\": [], \"memory\": []\n",
        "}\n",
        "\n",
        "def fwd(x, provider):\n",
        "    if provider == \"torch\":\n",
        "        return torch.softmax(x, axis=-1)\n",
        "    elif provider == 'triton':\n",
        "        return nn.Softmax()(x)\n",
        "    else:\n",
        "        raise ValueError\n",
        "\n",
        "def bwd(x, provider):\n",
        "    x.requires_grad = True\n",
        "    out = fwd(x, provider)\n",
        "    loss = out.sum()\n",
        "    loss.backward()\n",
        "\n",
        "def benchmark(fun_1, fun_2, tensors, N_try = 5):\n",
        "    report_list = []\n",
        "    for fun in [fun_1, fun_2]:\n",
        "        summary = {\"time\": [], \"memory\": []}\n",
        "        for x in tqdm(tensors, total=len(tensors)):\n",
        "            with Profiler() as p:\n",
        "                for _ in range(N_try):\n",
        "                    fun(x)\n",
        "            summary['time'].append(p.t_elapsed / N_try)\n",
        "            summary['memory'].append(p.peak_alloc)\n",
        "        report_list.append(summary)\n",
        "    return report_list\n",
        "\n",
        "report_list = benchmark(\n",
        "    fun_1 = lambda x : nn.Softmax()(x),\n",
        "    fun_2 = lambda x : torch.nn.functional.softmax(x),\n",
        "    tensors = [torch.rand(n,M) for n in [128 * i for i in range(2, 50, 5)]],\n",
        ")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5H8mpSxs6s8N",
      "metadata": {
        "id": "5H8mpSxs6s8N"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "Xq6puAFU5-hM",
      "metadata": {
        "id": "Xq6puAFU5-hM"
      },
      "outputs": [],
      "source": [
        "report_list"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7083158b",
      "metadata": {
        "id": "7083158b"
      },
      "outputs": [],
      "source": [
        "summary['time']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "rcIiknAF7ym9",
      "metadata": {
        "id": "rcIiknAF7ym9"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "o0HPvoaLvwvo",
      "metadata": {
        "id": "o0HPvoaLvwvo"
      },
      "outputs": [],
      "source": [
        "gelu_fwd = GELU()\n",
        "\n",
        "def naive_gelu(x:Tensor) -> Tensor:\n",
        "    const = math.sqrt(2 / math.pi)\n",
        "    angle = const * (x + 0.044715 * x * x * x)\n",
        "    tanh = torch.nn.functional.tanh(angle)\n",
        "    return 0.5 * x * (1 + tanh)\n",
        "\n",
        "\n",
        "def fwd(x, provider):\n",
        "    if provider == \"torch\":\n",
        "        return torch.nn.functional.gelu(x)#(x, axis=-1)\n",
        "    elif provider == 'triton':\n",
        "        return gelu_fwd(x)\n",
        "    else:\n",
        "        raise ValueError\n",
        "\n",
        "def bwd(x, provider):\n",
        "    x.requires_grad = True\n",
        "    out = fwd(x, provider)\n",
        "\n",
        "    loss = out.sum()\n",
        "    loss.backward()\n",
        "\n",
        "def measure_memory(f, *args, **kwargs):\n",
        "    torch.cuda.synchronize()\n",
        "    torch.cuda.reset_peak_memory_stats()\n",
        "    torch.cuda.empty_cache()\n",
        "\n",
        "    f(*args, **kwargs)  # run your function once\n",
        "\n",
        "    torch.cuda.synchronize()\n",
        "    peak = torch.cuda.max_memory_allocated() / 1e6  # MB\n",
        "    return peak\n",
        "\n",
        "\n",
        "base_benchmark_kwargs = {\n",
        "        \"x_names\":['N'],  # argument names to use as an x-axis for the plot\n",
        "        \"x_vals\":[128 * i for i in range(2, 50, 10)],  # different possible values for `x_name`\n",
        "        \"line_arg\":'provider',  # argument name whose value corresponds to a different line in the plot\n",
        "        \"line_vals\":['triton', 'torch'],  # possible values for `line_arg``\n",
        "        \"line_names\":[\"Triton\", \"Torch\"],  # label name for the lines\n",
        "        \"styles\":[('blue', '-'), ('green', '-')],  # line styles\n",
        "        \"ylabel\":\"GB/s\",  # label name for the y-axis\n",
        "        \"plot_name\":\"softmax\",  # name for the plot. Used also as a file name for saving the plot.\n",
        "        \"args\":{'M': 4096} # values for function arguments not in `x_names` and `y_name`\n",
        "}\n",
        "\n",
        "from copy import deepcopy\n",
        "configs = []\n",
        "for bench_kind in ['timing', 'memory']:\n",
        "    for mode in ['fwd', 'bwd']:\n",
        "        _kwargs = deepcopy(base_benchmark_kwargs)\n",
        "        _kwargs['args'].update({\"mode\" : mode})\n",
        "        _kwargs['args'].update({\"bench_kind\" : bench_kind})\n",
        "        _kwargs['plot_title'] += f' - {bench_kind} - {mode}'\n",
        "        configs.append(triton.testing.Benchmark(**_kwargs))\n",
        "\n",
        "\n",
        "def benchmark_fwd(x, provider):\n",
        "    if provider == \"torch\":\n",
        "        return torch.nn.functional.gelu(x)#(x, axis=-1)\n",
        "    elif provider == 'triton':\n",
        "        return gelu_fwd(x)\n",
        "    else:\n",
        "        raise ValueError(f\"provider must be either 'torch' or 'triron, got {provider}\")\n",
        "\n",
        "def bwd(x, provider):\n",
        "    x.requires_grad = True\n",
        "    out = benchmark_fwd(x, provider)\n",
        "    loss = out.sum()\n",
        "    loss.backward()\n",
        "\n",
        "MAP_FWD_BKW = {\n",
        "    \"fwd\" : benchmark_fwd,\n",
        "    \"bwd\" : bwd,\n",
        "}\n",
        "\n",
        "def measure_memory(f, *args, **kwargs):\n",
        "    torch.cuda.synchronize()\n",
        "    torch.cuda.reset_peak_memory_stats()\n",
        "    torch.cuda.empty_cache()\n",
        "\n",
        "    f(*args, **kwargs)  # run your function once\n",
        "\n",
        "    torch.cuda.synchronize()\n",
        "    peak = torch.cuda.max_memory_allocated() / 1e6  # MB\n",
        "    return peak\n",
        "\n",
        "def benchmark_kernel():\n",
        "\n",
        "    @triton.testing.perf_report(configs)\n",
        "    def benchmark(M, N, provider, mode, bench_kind):\n",
        "        x = torch.randn(M, N, device=DEVICE, dtype=torch.bfloat16)\n",
        "        stream = getattr(torch, DEVICE.type).Stream()\n",
        "        getattr(torch, DEVICE.type).set_stream(stream)\n",
        "        if bench_kind == \"timing\":\n",
        "            ms = triton.testing.do_bench(lambda: MAP_FWD_BKW[mode](x, provider))\n",
        "            gbps = lambda ms: 2 * x.numel() * x.element_size() * 1e-9 / (ms * 1e-3)\n",
        "            return gbps(ms)\n",
        "        elif bench_kind == \"memory\":\n",
        "            mem_mb = measure_memory(MAP_FWD_BKW[mode], x, provider)\n",
        "            return mem_mb\n",
        "        raise ValueError(f\"bench_kind must be either 'timing' or 'memory', got {bench_kind}\")\n",
        "\n",
        "    benchmark.run(show_plots=True, print_data=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "wlpDKph88-rv",
      "metadata": {
        "id": "wlpDKph88-rv"
      },
      "outputs": [],
      "source": [
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "cMj0H-sv70yS",
      "metadata": {
        "id": "cMj0H-sv70yS"
      },
      "outputs": [],
      "source": [
        "def measure_memory(f, *args, **kwargs):\n",
        "    torch.cuda.synchronize()\n",
        "    torch.cuda.reset_peak_memory_stats()\n",
        "    torch.cuda.empty_cache()\n",
        "\n",
        "    f(*args, **kwargs)  # run your function once\n",
        "\n",
        "    torch.cuda.synchronize()\n",
        "    peak = torch.cuda.max_memory_allocated() / 1e6  # MB\n",
        "    return peak\n",
        "\n",
        "def benchmark(M, N, provider):\n",
        "    x = torch.randn(M, N, device=DEVICE, dtype=torch.float32)\n",
        "\n",
        "    # --- timing ---\n",
        "    ms = triton.testing.do_bench(lambda: bwd(x, provider))\n",
        "\n",
        "    # --- memory usage ---\n",
        "    mem_mb = measure_memory(bwd, x, provider)\n",
        "\n",
        "    # --- throughput ---\n",
        "    gbps = 2 * x.numel() * x.element_size() * 1e-9 / (ms * 1e-3)\n",
        "\n",
        "    print(f\"{provider:<8} | Time: {ms:.3f} ms | Peak Memory: {mem_mb:.2f} MB\")\n",
        "    return gbps\n",
        "\n",
        "@triton.testing.perf_report(\n",
        "    triton.testing.Benchmark(\n",
        "        x_names=['N'],\n",
        "        x_vals=[128 * i for i in range(2, 100, 10)],\n",
        "        line_arg='provider',\n",
        "        line_vals=['triton', 'torch'],\n",
        "        line_names=[\"Triton\", \"Torch\"],\n",
        "        styles=[('blue', '-'), ('green', '-')],\n",
        "        ylabel=\"Peak Memory (MB)\",\n",
        "        plot_name=\"softmax-memory\",\n",
        "        args={'M': 4096},\n",
        "    )\n",
        ")\n",
        "def benchmark_memory(M, N, provider):\n",
        "    x = torch.randn(M, N, device=DEVICE, dtype=torch.float32)\n",
        "    mem_mb = measure_memory(bwd, x, provider)\n",
        "    return mem_mb\n",
        "\n",
        "benchmark_memory.run(show_plots=True, print_data=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9JmqnyDWw3i4",
      "metadata": {
        "id": "9JmqnyDWw3i4"
      },
      "outputs": [],
      "source": [
        "@triton.jit\n",
        "def fused_softmax_kernel(x_pointer, y_pointer, x_stride, y_stride, n_rows, n_cols, block_size: tl.constexpr):\n",
        "    # get the program id: each program of the grid handles one (or more) rows of the tensor\n",
        "    pid = tl.program_id(axis=0)\n",
        "\n",
        "    # strided execution: can run the program in a strided way (e.g. for row 0, 8, 16, ...)\n",
        "    row_step = tl.num_programs(axis=0) # n. of programs running on given axis\n",
        "\n",
        "    # loop through the rows executed by program with this pid\n",
        "    for row_idx in tl.range(pid, n_rows, row_step):\n",
        "        x_row_pointer = x_pointer + row_idx * x_stride\n",
        "\n",
        "        col_offset = tl.arange(0, block_size)\n",
        "        x_col_pointer = x_row_pointer + col_offset\n",
        "\n",
        "        # Create a mask to guard memory operations against out-of-bounds accesses.\n",
        "        mask = col_offset < n_cols\n",
        "\n",
        "        # compute the softmax (with shift for numerical stab.)\n",
        "        row = tl.load(x_col_pointer, mask, other=-float('inf'))\n",
        "\n",
        "        row_minus_max = row - tl.max(row, axis=0)\n",
        "        num = tl.exp(row_minus_max)\n",
        "        den = tl.sum(num, axis=0)\n",
        "        y = num / den\n",
        "\n",
        "        y_row_pointer = y_pointer + row_idx * y_stride\n",
        "        y_col_pointer = y_row_pointer + col_offset\n",
        "        tl.store(y_col_pointer, y, mask)\n",
        "\n",
        "\n",
        "def fused_softmax_triton(x:Tensor, block_size:int=1024) -> Tensor:\n",
        "    assert x.is_cuda\n",
        "\n",
        "    n_rows, n_cols = x.shape\n",
        "    y = torch.empty_like(x)\n",
        "    grid = n_rows,\n",
        "    BLOCK_SIZE = triton.next_power_of_2(n_cols)  # Used to tile the row\n",
        "\n",
        "    fused_softmax_kernel[grid](x, y, x.stride(0), y.stride(0), n_rows, n_cols, BLOCK_SIZE)\n",
        "\n",
        "    return y\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "941a1e0d",
      "metadata": {
        "id": "941a1e0d"
      },
      "outputs": [],
      "source": [
        "x = torch.rand(8, 16).to(DEVICE)\n",
        "assert torch.allclose(naive_softmax(x), fused_softmax_triton(x))\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "-NfWDsVPUoCK",
      "metadata": {
        "id": "-NfWDsVPUoCK"
      },
      "outputs": [],
      "source": [
        "@triton.testing.perf_report(\n",
        "    triton.testing.Benchmark(\n",
        "        x_names=['N'],  # argument names to use as an x-axis for the plot\n",
        "        x_vals=[128 * i for i in range(2, 100)],  # different possible values for `x_name`\n",
        "        line_arg='provider',  # argument name whose value corresponds to a different line in the plot\n",
        "        line_vals=['triton', 'torch', 'naive_softmax'],  # possible values for `line_arg``\n",
        "        line_names=[\"Triton\", \"Torch\", \"Naive Softmax\"],  # label name for the lines\n",
        "        styles=[('blue', '-'), ('green', '-'), ('red', '-')],  # line styles\n",
        "        ylabel=\"GB/s\",  # label name for the y-axis\n",
        "        plot_name=\"softmax-performance\",  # name for the plot. Used also as a file name for saving the plot.\n",
        "        args={'M': 4096},  # values for function arguments not in `x_names` and `y_name`\n",
        "    ))\n",
        "def benchmark(M, N, provider):\n",
        "    x = torch.randn(M, N, device=DEVICE, dtype=torch.float32)\n",
        "    stream = getattr(torch, DEVICE.type).Stream()\n",
        "    getattr(torch, DEVICE.type).set_stream(stream)\n",
        "    if provider == 'torch':\n",
        "        ms = triton.testing.do_bench(lambda: torch.softmax(x, axis=-1))\n",
        "    if provider == 'triton':\n",
        "        ms = triton.testing.do_bench(lambda: fused_softmax_triton(x))\n",
        "    if provider == 'naive_softmax':\n",
        "        ms = triton.testing.do_bench(lambda: naive_softmax(x))\n",
        "    gbps = lambda ms: 2 * x.numel() * x.element_size() * 1e-9 / (ms * 1e-3)\n",
        "    return gbps(ms)\n",
        "\n",
        "\n",
        "benchmark.run(show_plots=True, print_data=True)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}