{
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "os.environ['CUDA_LAUNCH_BLOCKING'] = '1'"
      ],
      "metadata": {
        "id": "oxMW2XlKqUG7"
      },
      "id": "oxMW2XlKqUG7",
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "YlFBDzu-LL6v",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YlFBDzu-LL6v",
        "outputId": "5fd82c09-e82a-40e9-84ad-5e5762310fc8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "cuda\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "from torch import Tensor, nn\n",
        "from math import ceil\n",
        "from torch.autograd import Function\n",
        "# import os\n",
        "from torch.autograd.function import Function\n",
        "import triton\n",
        "from copy import deepcopy\n",
        "import triton.language as tl\n",
        "\n",
        "DEVICE = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
        "print(DEVICE)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "hRyr14v0wSwN",
      "metadata": {
        "id": "hRyr14v0wSwN"
      },
      "outputs": [],
      "source": [
        "import math\n",
        "\n",
        "def validate_contiguous(x: Tensor) -> Tensor:\n",
        "    return x if x.is_contiguous() else x.contiguous()\n",
        "\n",
        "\n",
        "def validate_tensor_device(x: Tensor):\n",
        "    if not x.is_cuda:\n",
        "        message = \"Tensor must be on CUDA or TRITON_INTERPRET must be set to '1'\"\n",
        "        assert os.environ.get(\"TRITON_INTERPRET\", False) == \"1\", message\n",
        "\n",
        "\n",
        "def is_cuda_available() -> bool:\n",
        "    return torch.cuda.is_available()\n",
        "\n",
        "\n",
        "def get_device() -> torch.device:\n",
        "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "    return torch.device(device)\n",
        "\n",
        "\n",
        "\n",
        "@triton.jit\n",
        "def _gelu_fwd_triton(\n",
        "    x_ptr, tanh_ptr, phi_ptr, act_ptr, num_elements, block_size: tl.constexpr\n",
        "):\n",
        "\n",
        "    pid = tl.program_id(axis=0)\n",
        "    ptr_offset = pid * block_size + tl.arange(0, block_size)\n",
        "\n",
        "    mask = ptr_offset < num_elements\n",
        "\n",
        "    x = tl.load(x_ptr + ptr_offset, mask)\n",
        "\n",
        "    angle = tl.sqrt(2 / math.pi) * (x + 0.044715 * x * x * x)\n",
        "    exp = tl.exp(2 * angle)\n",
        "    tanh = (exp - 1) / (exp + 1)\n",
        "    phi = 0.5 * (1 + tanh)\n",
        "    chunk_act = x * phi\n",
        "\n",
        "    tl.store(tanh_ptr + ptr_offset, tanh, mask)\n",
        "    tl.store(phi_ptr + ptr_offset, phi, mask)\n",
        "    tl.store(act_ptr + ptr_offset, chunk_act, mask)\n",
        "\n",
        "\n",
        "def _gelu_fwd(x: Tensor, block_size: int = 2048) -> Tensor:\n",
        "    validate_tensor_device(x)\n",
        "\n",
        "    num_elements = x.numel()\n",
        "    grid = (ceil(num_elements / block_size),)\n",
        "    act = torch.empty_like(x).to(x.device)\n",
        "    phi = torch.empty_like(x).to(x.device)\n",
        "    tanh = torch.empty_like(x).to(x.device)\n",
        "\n",
        "    _gelu_fwd_triton[grid](x, tanh, phi, act, num_elements, block_size)\n",
        "    return act, tanh, phi\n",
        "\n",
        "\n",
        "@triton.jit\n",
        "def _gelu_bwd_triton(\n",
        "    x_ptr, tanh_ptr, phi_ptr, derivative_ptr, num_elements, block_size: tl.constexpr\n",
        "):\n",
        "\n",
        "    pid = tl.program_id(axis=0)\n",
        "    ptr_offset = pid * block_size + tl.arange(0, block_size)\n",
        "    mask = ptr_offset < num_elements\n",
        "\n",
        "    x = tl.load(x_ptr + ptr_offset, mask)\n",
        "    tanh = tl.load(tanh_ptr + ptr_offset, mask)\n",
        "    phi = tl.load(phi_ptr + ptr_offset, mask)\n",
        "\n",
        "    factor = 1 / tl.sqrt(2 * math.pi)\n",
        "    # angle = tl.sqrt(2 / pi) * (x + 0.044715 * x ** 3)\n",
        "    # tanh = nn.functional.tanh(gx)\n",
        "    phi_prime = factor * (1 - tanh * tanh) * (1 + 3 * 0.044715 * x * x)\n",
        "\n",
        "    # phi = 0.5 * (1 + tanh)\n",
        "\n",
        "    derivative = x * phi_prime + phi\n",
        "\n",
        "    tl.store(derivative_ptr + ptr_offset, derivative, mask)\n",
        "\n",
        "\n",
        "def _gelu_bwd(x: Tensor, tanh: Tensor, phi: Tensor, block_size: int = 2048) -> Tensor:\n",
        "    for t in [x, tanh, phi]:\n",
        "        validate_tensor_device(t)\n",
        "\n",
        "    num_elements = x.numel()\n",
        "    grid = (ceil(num_elements / block_size),)\n",
        "\n",
        "    derivative = torch.empty_like(x).to(x.device)\n",
        "\n",
        "    _gelu_bwd_triton[grid](x, tanh, phi, derivative, num_elements, block_size)\n",
        "\n",
        "    return derivative\n",
        "\n",
        "\n",
        "class GeluFunction(Function):\n",
        "    @staticmethod\n",
        "    def forward(ctx, x: Tensor):\n",
        "        act, tanh, phi = _gelu_fwd(x)\n",
        "        ctx.save_for_backward(x, tanh, phi)\n",
        "        # return _gelu(x)\n",
        "        return act\n",
        "\n",
        "    @staticmethod\n",
        "    def backward(ctx, grad_output):\n",
        "        x, tanh, phi = ctx.saved_tensors\n",
        "\n",
        "        # factor = 1 / (2 * math.pi) ** 0.5\n",
        "        # gx = (2 / math.pi) ** 0.5 * (x + 0.044715 * x ** 3)\n",
        "        # tanh = nn.functional.tanh(gx)\n",
        "        # phi_prime = factor * (1 - tanh**2) * (1 + 3 * 0.044715 * x ** 2)\n",
        "        # phi = 0.5 * (1 + tanh)\n",
        "\n",
        "        # derivative = phi + x * phi_prime\n",
        "        derivative = _gelu_bwd(x, tanh, phi)\n",
        "        return derivative * grad_output\n",
        "\n",
        "\n",
        "class GELU(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "\n",
        "    def forward(self, x: Tensor) -> Tensor:\n",
        "        if x.ndim > 1:\n",
        "            x = validate_contiguous(x)\n",
        "            return GeluFunction.apply(x.view(-1)).view(x.shape)\n",
        "        return GeluFunction.apply(x)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "\n",
        "@triton.jit\n",
        "def _compute_tanh_phi(x):\n",
        "    angle = tl.sqrt(2 / math.pi) * (x + .044715 * x * x * x)\n",
        "    exp = tl.exp(2 * angle)\n",
        "    tanh = (exp - 1) / (exp + 1)\n",
        "    phi = 0.5 * (1 + tanh)\n",
        "    return tanh, phi\n",
        "\n",
        "@triton.jit\n",
        "def _gelu_fwd_triton(\n",
        "    x_ptr, act_ptr, num_elements, block_size: tl.constexpr\n",
        "):\n",
        "\n",
        "    pid = tl.program_id(axis=0)\n",
        "    ptr_offset = pid * block_size + tl.arange(0, block_size)\n",
        "    mask = ptr_offset < num_elements\n",
        "    x = tl.load(x_ptr + ptr_offset, mask)\n",
        "\n",
        "    _, phi = _compute_tanh_phi(x)\n",
        "    chunk_act = x * phi\n",
        "\n",
        "    tl.store(act_ptr + ptr_offset, chunk_act, mask)\n",
        "\n",
        "\n",
        "def _gelu_fwd(x: Tensor, block_size: int = 2048) -> Tensor:\n",
        "    validate_tensor_device(x)\n",
        "\n",
        "    num_elements = x.numel()\n",
        "    grid = (ceil(num_elements / block_size),)\n",
        "    act = torch.empty_like(x).to(x.device)\n",
        "\n",
        "    _gelu_fwd_triton[grid](x, act, num_elements, block_size)\n",
        "    return act #, tanh, phi\n",
        "\n",
        "\n",
        "@triton.jit\n",
        "def _gelu_bwd_triton(\n",
        "    x_ptr, grad_input_ptr, grad_output_ptr, num_elements, block_size: tl.constexpr\n",
        "):\n",
        "\n",
        "    pid = tl.program_id(axis=0)\n",
        "    ptr_offset = pid * block_size + tl.arange(0, block_size)\n",
        "    mask = ptr_offset < num_elements\n",
        "\n",
        "    x = tl.load(x_ptr + ptr_offset, mask)\n",
        "    grad_output = tl.load(grad_output_ptr + ptr_offset, mask)\n",
        "\n",
        "    tanh, phi = _compute_tanh_phi(x)\n",
        "\n",
        "    factor = 1 / tl.sqrt(2 * math.pi)\n",
        "    phi_prime = factor * (1 - tanh * tanh) * (1 + 3 * 0.044715 * x * x)\n",
        "\n",
        "    derivative = x * phi_prime + phi\n",
        "    grad_input = derivative * grad_output\n",
        "\n",
        "    tl.store(grad_input_ptr + ptr_offset, grad_input, mask)\n",
        "\n",
        "\n",
        "# def _gelu_bwd(x: Tensor, grad_output: Tensor, block_size: int = 2048) -> Tensor:\n",
        "#     # for t in [x, tanh, phi]:\n",
        "#     validate_tensor_device(x)\n",
        "\n",
        "#     num_elements = x.numel()\n",
        "#     grid = (ceil(num_elements / block_size),)\n",
        "\n",
        "#     grad_input = torch.empty_like(x).to(x.device)\n",
        "\n",
        "#     _gelu_bwd_triton[grid](x, grad_input, grad_output, num_elements, block_size)\n",
        "\n",
        "#     return grad_input\n",
        "\n",
        "def _gelu_bwd(x: Tensor, grad_output: Tensor, block_size: int = 2048) -> Tensor:\n",
        "    # Aggiungi questi controlli\n",
        "    validate_tensor_device(x)\n",
        "\n",
        "    assert x.numel() == grad_output.numel(), \"Expected x.numel() = grad_output.numel()\"\n",
        "    assert x.is_contiguous(), f\"x not contiguous, {x.stride()=}\"\n",
        "    assert grad_output.is_contiguous(), f\"grad_output not contiguous, {grad_output.stride()=}\"\n",
        "\n",
        "    num_elements = x.numel()\n",
        "    grid = (ceil(num_elements / block_size),)\n",
        "\n",
        "    grad_input = torch.empty_like(x).to(x.device)\n",
        "\n",
        "    _gelu_bwd_triton[grid](\n",
        "        x, grad_output, grad_input,\n",
        "        num_elements,\n",
        "        block_size\n",
        "    )\n",
        "\n",
        "\n",
        "@triton.jit\n",
        "def _gelu_bwd_strided_triton(\n",
        "    x_ptr, grad_output_ptr, grad_input_ptr,\n",
        "    num_elements,\n",
        "    # Stride per 'x'\n",
        "    x_stride_row, x_stride_col,\n",
        "    # Stride per 'grad_output'\n",
        "    grad_output_stride_row, grad_output_stride_col,\n",
        "    # Stride per 'grad_input'\n",
        "    grad_input_stride_row, grad_input_stride_col,\n",
        "    # Dimensione finale per il calcolo degli indici\n",
        "    COL_DIM_SIZE: tl.constexpr,\n",
        "    block_size: tl.constexpr\n",
        "):\n",
        "    pid = tl.program_id(axis=0)\n",
        "\n",
        "    # Blocco di offset logici 1D\n",
        "    offsets_1d = pid * block_size + tl.arange(0, block_size)\n",
        "    mask = offsets_1d < num_elements\n",
        "\n",
        "    # Calcola gli indici logici 2D\n",
        "    row_idx = offsets_1d // COL_DIM_SIZE\n",
        "    col_idx = offsets_1d % COL_DIM_SIZE\n",
        "\n",
        "    # Calcola gli offset di memoria\n",
        "    x_offsets = row_idx * x_stride_row + col_idx * x_stride_col\n",
        "    grad_output_offsets = row_idx * grad_output_stride_row + col_idx * grad_output_stride_col\n",
        "    grad_input_offsets = row_idx * grad_input_stride_row + col_idx * grad_input_stride_col\n",
        "\n",
        "    # Carica usando gli offset calcolati\n",
        "    x = tl.load(x_ptr + x_offsets, mask)\n",
        "    grad_output = tl.load(grad_output_ptr + grad_output_offsets, mask)\n",
        "\n",
        "    # Il resto della logica Ã¨ identico\n",
        "    tanh, phi = _compute_tanh_phi(x)\n",
        "    factor = 1 / tl.sqrt(2 * math.pi)\n",
        "    phi_prime = factor * (1 - tanh * tanh) * (1 + 3 * 0.044715 * x * x)\n",
        "    derivative = x * phi_prime + phi\n",
        "    grad_input = derivative * grad_output\n",
        "\n",
        "    # Scrivi usando gli offset calcolati\n",
        "    tl.store(grad_input_ptr + grad_input_offsets, grad_input, mask)\n",
        "\n",
        "def _gelu_bwd_strided(x: Tensor, grad_output: Tensor, block_size: int = 2048) -> Tensor:\n",
        "    validate_tensor_device(x)\n",
        "\n",
        "    # Assicurati che le forme logiche corrispondano\n",
        "    assert x.shape == grad_output.shape, \"Le forme di x e grad_output non corrispondono\"\n",
        "\n",
        "    # Assumiamo 2D per questo esempio, ma puoi generalizzarlo\n",
        "    assert x.ndim == 2, \"Questo kernel di esempio funziona solo per tensori 2D\"\n",
        "\n",
        "    num_elements = x.numel()\n",
        "    grid = (ceil(num_elements / block_size),)\n",
        "\n",
        "    # Crea un output con gli stessi stride di 'x' (un'ipotesi comune)\n",
        "    # Per una correttezza totale, dovresti decidere quale layout vuoi per l'output.\n",
        "    # Ma `empty_like` di solito preserva il layout.\n",
        "    grad_input = torch.empty_like(x).to(x.device)\n",
        "\n",
        "    # Estrai forme e stride\n",
        "    M, N = x.shape\n",
        "    x_stride_row, x_stride_col = x.stride()\n",
        "    grad_output_stride_row, grad_output_stride_col = grad_output.stride()\n",
        "    grad_input_stride_row, grad_input_stride_col = grad_input.stride()\n",
        "\n",
        "    _gelu_bwd_strided_triton[grid](\n",
        "        x, grad_output, grad_input,\n",
        "        num_elements,\n",
        "        x_stride_row, x_stride_col,\n",
        "        grad_output_stride_row, grad_output_stride_col,\n",
        "        grad_input_stride_row, grad_input_stride_col,\n",
        "        COL_DIM_SIZE=N,  # Passa la dimensione della colonna come constexpr\n",
        "        block_size=block_size\n",
        "    )\n",
        "\n",
        "    return grad_input\n",
        "\n",
        "class GeluFunction(Function):\n",
        "    @staticmethod\n",
        "    def forward(ctx, x: Tensor):\n",
        "        act = _gelu_fwd(x)\n",
        "        ctx.save_for_backward(x)\n",
        "        # return _gelu(x)\n",
        "        return act\n",
        "\n",
        "    @staticmethod\n",
        "    def backward(ctx, grad_output):\n",
        "        x,  = ctx.saved_tensors\n",
        "\n",
        "        # factor = 1 / (2 * math.pi) ** 0.5\n",
        "        # gx = (2 / math.pi) ** 0.5 * (x + 0.044715 * x ** 3)\n",
        "        # tanh = nn.functional.tanh(gx)\n",
        "        # phi_prime = factor * (1 - tanh**2) * (1 + 3 * 0.044715 * x ** 2)\n",
        "        # phi = 0.5 * (1 + tanh)\n",
        "\n",
        "        # derivative = phi + x * phi_prime\n",
        "        # x = validate_contiguous(x)\n",
        "        grad_output = validate_contiguous(grad_output)\n",
        "        grad_input = _gelu_bwd(x, grad_output)\n",
        "        return grad_input\n",
        "\n",
        "\n",
        "class GELU(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "\n",
        "    def forward(self, x: Tensor) -> Tensor:\n",
        "        if x.ndim > 1:\n",
        "            x = validate_contiguous(x)\n",
        "            return GeluFunction.apply(x.view(-1)).view(x.shape)\n",
        "        return GeluFunction.apply(x)\n"
      ],
      "metadata": {
        "id": "LTkjtiN_fFXN"
      },
      "id": "LTkjtiN_fFXN",
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "base_benchmark_kwargs = {\n",
        "    \"x_names\":['N'],  # argument names to use as an x-axis for the plot\n",
        "    \"x_vals\":[128 * i for i in range(2, 100, 10)],  # different possible values for `x_name`\n",
        "    \"line_arg\":'provider',  # argument name whose value corresponds to a different line in the plot\n",
        "    \"line_vals\":['triton', 'torch'],  # possible values for `line_arg``\n",
        "    \"line_names\":[\"Triton\", \"Torch\"],  # label name for the lines\n",
        "    \"plot_name\":\"gelu\",  # name for the plot. Used also as a file name for saving the plot.\n",
        "    \"args\":{'M': 4096} # values for function arguments not in `x_names` and `y_name`\n",
        "}\n",
        "\n",
        "_fwd = GELU()\n",
        "\n",
        "def fwd(x, provider):\n",
        "    ### for benchmark olny!\n",
        "    if provider == \"torch\":\n",
        "        return torch.nn.functional.gelu(x)\n",
        "    elif provider == 'triton':\n",
        "        return _fwd(x)\n",
        "    else:\n",
        "        raise ValueError\n"
      ],
      "metadata": {
        "id": "JJJm-rICoD0g"
      },
      "id": "JJJm-rICoD0g",
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "9453daf8",
      "metadata": {
        "id": "9453daf8",
        "outputId": "9c958874-03a7-4328-e631-dde3f340f40a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 180
        }
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'fwd' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-2962793252.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     55\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mresult_dfs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     56\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 57\u001b[0;31m \u001b[0mresult_dfs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbenchmark_kernel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfwd\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbase_benchmark_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m: name 'fwd' is not defined"
          ]
        }
      ],
      "source": [
        "def measure_memory(f, *args, **kwargs):\n",
        "    torch.cuda.synchronize()\n",
        "    torch.cuda.reset_peak_memory_stats()\n",
        "    torch.cuda.empty_cache()\n",
        "\n",
        "    f(*args, **kwargs)  # run your function once\n",
        "\n",
        "    torch.cuda.synchronize()\n",
        "    peak = torch.cuda.max_memory_allocated() / 1e6  # MB\n",
        "    return peak\n",
        "\n",
        "def benchmark_kernel(fwd, base_benchmark_kwargs):\n",
        "\n",
        "    def bwd(x, provider):\n",
        "        x.requires_grad = True\n",
        "        out = fwd(x, provider)\n",
        "        loss = out.sum()\n",
        "        loss.backward()\n",
        "\n",
        "\n",
        "    MAP_FWD_BKW = {\n",
        "        \"fwd\" : fwd,\n",
        "        \"bwd\" : bwd,\n",
        "    }\n",
        "\n",
        "    configs = []\n",
        "    for bench_kind in ['timing', 'memory']:\n",
        "        for mode in ['fwd', 'bwd']:\n",
        "            _kwargs = deepcopy(base_benchmark_kwargs)\n",
        "            _kwargs['args'].update({\"mode\" : mode})\n",
        "            _kwargs['args'].update({\"bench_kind\" : bench_kind})\n",
        "            _kwargs['ylabel'] = 'ms' if bench_kind == 'timing' else 'MB'\n",
        "            _kwargs['plot_name'] += f' - {bench_kind} - {mode}'\n",
        "            configs.append(triton.testing.Benchmark(**_kwargs))\n",
        "\n",
        "    @triton.testing.perf_report(configs)\n",
        "    def benchmark(M, N, provider, mode, bench_kind):\n",
        "        print(f\"Running benchmark: {mode=}, {bench_kind=}\")\n",
        "        x = torch.randn(M, N, device=DEVICE, dtype=torch.float32) # Changed dtype to float32 for debugging\n",
        "        stream = getattr(torch, DEVICE.type).Stream()\n",
        "        getattr(torch, DEVICE.type).set_stream(stream)\n",
        "        if bench_kind == \"timing\":\n",
        "            ms = triton.testing.do_bench(lambda: MAP_FWD_BKW[mode](x, provider))\n",
        "            # gbps = lambda ms: 2 * x.numel() * x.element_size() * 1e-9 / (ms * 1e-3)\n",
        "            return ms #gbps(ms)\n",
        "        elif bench_kind == \"memory\":\n",
        "            mem_mb = measure_memory(MAP_FWD_BKW[mode], x, provider)\n",
        "            return mem_mb\n",
        "        raise ValueError(f\"bench_kind must be either 'timing' or 'memory', got {bench_kind}\")\n",
        "\n",
        "    result_dfs = benchmark.run(show_plots=False, print_data=False, return_df=True)\n",
        "    for df, config in zip(result_dfs, configs):\n",
        "        df.plot(x='N', ylabel = config.ylabel, title=config.plot_name, legend=True)\n",
        "\n",
        "    return result_dfs\n",
        "\n",
        "result_dfs = benchmark_kernel(fwd, base_benchmark_kwargs)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def measure_memory(f, *args, **kwargs):\n",
        "    torch.cuda.synchronize()\n",
        "    torch.cuda.reset_peak_memory_stats()\n",
        "    torch.cuda.empty_cache()\n",
        "\n",
        "    f(*args, **kwargs)  # run your function once\n",
        "\n",
        "    torch.cuda.synchronize()\n",
        "    peak = torch.cuda.max_memory_allocated() / 1e6  # MB\n",
        "    return peak\n",
        "\n",
        "def benchmark_kernel(fwd, base_benchmark_kwargs):\n",
        "\n",
        "    def bwd(x, provider):\n",
        "        x.requires_grad = True\n",
        "        out = fwd(x, provider)\n",
        "        loss = out.sum()\n",
        "        loss.backward()\n",
        "\n",
        "\n",
        "    MAP_FWD_BKW = {\n",
        "        \"fwd\" : fwd,\n",
        "        \"bwd\" : bwd,\n",
        "    }\n",
        "\n",
        "    configs = []\n",
        "    for bench_kind in ['timing', 'memory']:\n",
        "        for mode in ['fwd', 'bwd']:\n",
        "            _kwargs = deepcopy(base_benchmark_kwargs)\n",
        "            _kwargs['args'].update({\"mode\" : mode})\n",
        "            _kwargs['args'].update({\"bench_kind\" : bench_kind})\n",
        "            _kwargs['ylabel'] = 'ms' if bench_kind == 'timing' else 'MB'\n",
        "            _kwargs['plot_name'] += f' - {bench_kind} - {mode}'\n",
        "            configs.append(triton.testing.Benchmark(**_kwargs))\n",
        "\n",
        "    @triton.testing.perf_report(configs)\n",
        "    def benchmark(M, N, provider, mode, bench_kind):\n",
        "        x = torch.randn(M, N, device=DEVICE, dtype=torch.bfloat16)\n",
        "        stream = getattr(torch, DEVICE.type).Stream()\n",
        "        getattr(torch, DEVICE.type).set_stream(stream)\n",
        "        if bench_kind == \"timing\":\n",
        "            ms = triton.testing.do_bench(lambda: MAP_FWD_BKW[mode](x, provider))\n",
        "            # gbps = lambda ms: 2 * x.numel() * x.element_size() * 1e-9 / (ms * 1e-3)\n",
        "            return ms #gbps(ms)\n",
        "        elif bench_kind == \"memory\":\n",
        "            mem_mb = measure_memory(MAP_FWD_BKW[mode], x, provider)\n",
        "            return mem_mb\n",
        "        raise ValueError(f\"bench_kind must be either 'timing' or 'memory', got {bench_kind}\")\n",
        "\n",
        "    result_dfs = benchmark.run(show_plots=False, print_data=False, return_df=True)\n",
        "    for df, config in zip(result_dfs, configs):\n",
        "        df.plot(x='N', ylabel = config.ylabel, title=config.plot_name, legend=True)\n",
        "\n",
        "    return result_dfs\n",
        "\n",
        "result_dfs = benchmark_kernel(fwd, base_benchmark_kwargs)"
      ],
      "metadata": {
        "id": "4PeW9r5sfHjh",
        "outputId": "7a07cb5f-dbe4-4563-b747-33d17ff97271",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 382
        }
      },
      "id": "4PeW9r5sfHjh",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "AcceleratorError",
          "evalue": "CUDA error: an illegal memory access was encountered\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAcceleratorError\u001b[0m                          Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-671603793.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     54\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mresult_dfs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     55\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 56\u001b[0;31m \u001b[0mresult_dfs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbenchmark_kernel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfwd\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbase_benchmark_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/tmp/ipython-input-671603793.py\u001b[0m in \u001b[0;36mbenchmark_kernel\u001b[0;34m(fwd, base_benchmark_kwargs)\u001b[0m\n\u001b[1;32m     48\u001b[0m         \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"bench_kind must be either 'timing' or 'memory', got {bench_kind}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     49\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 50\u001b[0;31m     \u001b[0mresult_dfs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbenchmark\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mshow_plots\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprint_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreturn_df\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     51\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconfig\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresult_dfs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconfigs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     52\u001b[0m         \u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'N'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mylabel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mylabel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtitle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplot_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlegend\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/triton/testing.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, show_plots, print_data, save_path, return_df, **kwargs)\u001b[0m\n\u001b[1;32m    390\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    391\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mbench\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mbenchmarks\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 392\u001b[0;31m                 \u001b[0mresult_dfs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_run\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbench\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msave_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshow_plots\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprint_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    393\u001b[0m         \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    394\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0msave_path\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/triton/testing.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, bench, save_path, show_plots, print_data, diff_col, save_precision, **kwrags)\u001b[0m\n\u001b[1;32m    337\u001b[0m             \u001b[0mrow_mean\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrow_min\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrow_max\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    338\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0my\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mbench\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mline_vals\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 339\u001b[0;31m                 \u001b[0mret\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mx_args\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0mbench\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mline_arg\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mbench\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwrags\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    340\u001b[0m                 \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    341\u001b[0m                     \u001b[0my_mean\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_min\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_max\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mret\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tmp/ipython-input-671603793.py\u001b[0m in \u001b[0;36mbenchmark\u001b[0;34m(M, N, provider, mode, bench_kind)\u001b[0m\n\u001b[1;32m     36\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mtriton\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtesting\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mperf_report\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconfigs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     37\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mbenchmark\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mM\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mN\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprovider\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbench_kind\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 38\u001b[0;31m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mM\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mN\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mDEVICE\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbfloat16\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     39\u001b[0m         \u001b[0mstream\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mDEVICE\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mStream\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     40\u001b[0m         \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mDEVICE\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_stream\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstream\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mAcceleratorError\u001b[0m: CUDA error: an illegal memory access was encountered\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "61f7ff28",
      "metadata": {
        "id": "61f7ff28"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "XET5-tKKb1tB"
      },
      "id": "XET5-tKKb1tB",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}