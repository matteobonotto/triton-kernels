{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "YlFBDzu-LL6v",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YlFBDzu-LL6v",
        "outputId": "9a01ee72-d4a6-4e6c-e914-a7099d1929df"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "cuda\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "from torch import Tensor, nn\n",
        "from math import ceil\n",
        "from torch.autograd import Function\n",
        "import os\n",
        "from torch.autograd.function import Function\n",
        "import triton\n",
        "from copy import deepcopy\n",
        "import triton.language as tl\n",
        "\n",
        "DEVICE = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
        "print(DEVICE)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "hRyr14v0wSwN",
      "metadata": {
        "id": "hRyr14v0wSwN"
      },
      "outputs": [],
      "source": [
        "\n",
        "def validate_contiguous(x: Tensor) -> Tensor:\n",
        "    return x if x.is_contiguous() else x.contiguous()\n",
        "\n",
        "\n",
        "def validate_tensor_device(x: Tensor):\n",
        "    if not x.is_cuda:\n",
        "        message = \"Tensor must be on CUDA or TRITON_INTERPRET must be set to '1'\"\n",
        "        assert os.environ.get(\"TRITON_INTERPRET\", False) == \"1\", message\n",
        "\n",
        "\n",
        "def is_cuda_available() -> bool:\n",
        "    return torch.cuda.is_available()\n",
        "\n",
        "\n",
        "def get_device() -> torch.device:\n",
        "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "    return torch.device(device)\n",
        "\n",
        "\n",
        "@triton.jit\n",
        "def _gelu_fwd_triton(\n",
        "    x_ptr, tanh_ptr, phi_ptr, act_ptr, num_elements, block_size: tl.constexpr\n",
        "):\n",
        "\n",
        "    pid = tl.program_id(axis=0)\n",
        "    ptr_offset = pid * block_size + tl.arange(0, block_size)\n",
        "\n",
        "    mask = ptr_offset < num_elements\n",
        "\n",
        "    x = tl.load(x_ptr + ptr_offset, mask)\n",
        "\n",
        "    angle = tl.sqrt(2 / pi) * (x + 0.044715 * x * x * x)\n",
        "    exp = tl.exp(2 * angle)\n",
        "    tanh = (exp - 1) / (exp + 1)\n",
        "    phi = 0.5 * (1 + tanh)\n",
        "    chunk_act = x * phi\n",
        "\n",
        "    tl.store(tanh_ptr + ptr_offset, tanh, mask)\n",
        "    tl.store(phi_ptr + ptr_offset, phi, mask)\n",
        "    tl.store(act_ptr + ptr_offset, chunk_act, mask)\n",
        "\n",
        "\n",
        "def _gelu_fwd(x: Tensor, block_size: int = 2048) -> Tensor:\n",
        "    validate_tensor_device(x)\n",
        "\n",
        "    num_elements = x.numel()\n",
        "    grid = (ceil(num_elements / block_size),)\n",
        "    act = torch.empty_like(x).to(x.device)\n",
        "    phi = torch.empty_like(x).to(x.device)\n",
        "    tanh = torch.empty_like(x).to(x.device)\n",
        "\n",
        "    _gelu_fwd_triton[grid](x, tanh, phi, act, num_elements, block_size)\n",
        "    return act, tanh, phi\n",
        "\n",
        "\n",
        "@triton.jit\n",
        "def _gelu_bwd_triton(\n",
        "    x_ptr, tanh_ptr, phi_ptr, derivative_ptr, num_elements, block_size: tl.constexpr\n",
        "):\n",
        "\n",
        "    pid = tl.program_id(axis=0)\n",
        "    ptr_offset = pid * block_size + tl.arange(0, block_size)\n",
        "    mask = ptr_offset < num_elements\n",
        "\n",
        "    x = tl.load(x_ptr + ptr_offset, mask)\n",
        "    tanh = tl.load(tanh_ptr + ptr_offset, mask)\n",
        "    phi = tl.load(phi_ptr + ptr_offset, mask)\n",
        "\n",
        "    factor = 1 / tl.sqrt(2 * pi)\n",
        "    # angle = tl.sqrt(2 / pi) * (x + 0.044715 * x ** 3)\n",
        "    # tanh = nn.functional.tanh(gx)\n",
        "    phi_prime = factor * (1 - tanh * tanh) * (1 + 3 * 0.044715 * x * x)\n",
        "\n",
        "    # phi = 0.5 * (1 + tanh)\n",
        "\n",
        "    derivative = x * phi_prime + phi\n",
        "\n",
        "    tl.store(derivative_ptr + ptr_offset, derivative, mask)\n",
        "\n",
        "\n",
        "def _gelu_bwd(x: Tensor, tanh: Tensor, phi: Tensor, block_size: int = 2048) -> Tensor:\n",
        "    for t in [x, tanh, phi]:\n",
        "        validate_tensor_device(t)\n",
        "\n",
        "    num_elements = x.numel()\n",
        "    grid = (ceil(num_elements / block_size),)\n",
        "\n",
        "    derivative = torch.empty_like(x).to(x.device)\n",
        "\n",
        "    _gelu_bwd_triton[grid](x, tanh, phi, derivative, num_elements, block_size)\n",
        "\n",
        "    return derivative\n",
        "\n",
        "\n",
        "class GeluFunction(Function):\n",
        "    @staticmethod\n",
        "    def forward(ctx, x: Tensor):\n",
        "        act, tanh, phi = _gelu_fwd(x)\n",
        "        ctx.save_for_backward(x, tanh, phi)\n",
        "        # return _gelu(x)\n",
        "        return act\n",
        "\n",
        "    @staticmethod\n",
        "    def backward(ctx, grad_output):\n",
        "        x, tanh, phi = ctx.saved_tensors\n",
        "\n",
        "        # factor = 1 / (2 * math.pi) ** 0.5\n",
        "        # gx = (2 / math.pi) ** 0.5 * (x + 0.044715 * x ** 3)\n",
        "        # tanh = nn.functional.tanh(gx)\n",
        "        # phi_prime = factor * (1 - tanh**2) * (1 + 3 * 0.044715 * x ** 2)\n",
        "        # phi = 0.5 * (1 + tanh)\n",
        "\n",
        "        # derivative = phi + x * phi_prime\n",
        "        derivative = _gelu_bwd(x, tanh, phi)\n",
        "        return derivative * grad_output\n",
        "\n",
        "\n",
        "class GELU(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "\n",
        "    def forward(self, x: Tensor) -> Tensor:\n",
        "        if x.ndim > 1:\n",
        "            x = validate_contiguous(x)\n",
        "            return GeluFunction.apply(x.view(-1)).view(x.shape)\n",
        "        return GeluFunction.apply(x)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "61f7ff28",
      "metadata": {},
      "outputs": [],
      "source": [
        "base_benchmark_kwargs = {\n",
        "    \"x_names\":['N'],  # argument names to use as an x-axis for the plot\n",
        "    \"x_vals\":[128 * i for i in range(2, 100, 10)],  # different possible values for `x_name`\n",
        "    \"line_arg\":'provider',  # argument name whose value corresponds to a different line in the plot\n",
        "    \"line_vals\":['triton', 'torch'],  # possible values for `line_arg``\n",
        "    \"line_names\":[\"Triton\", \"Torch\"],  # label name for the lines\n",
        "    \"plot_name\":\"gelu\",  # name for the plot. Used also as a file name for saving the plot.\n",
        "    \"args\":{'M': 4096} # values for function arguments not in `x_names` and `y_name`\n",
        "}\n",
        "\n",
        "_fwd = GELU()\n",
        "\n",
        "def fwd(x, provider):\n",
        "    ### for benchmark olny! \n",
        "    if provider == \"torch\":\n",
        "        return torch.nn.functional.gelu(x)\n",
        "    elif provider == 'triton':\n",
        "        return _fwd(x)\n",
        "    else:\n",
        "        raise ValueError\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9453daf8",
      "metadata": {},
      "outputs": [],
      "source": [
        "def measure_memory(f, *args, **kwargs):\n",
        "    torch.cuda.synchronize()\n",
        "    torch.cuda.reset_peak_memory_stats()\n",
        "    torch.cuda.empty_cache()\n",
        "\n",
        "    f(*args, **kwargs)  # run your function once\n",
        "\n",
        "    torch.cuda.synchronize()\n",
        "    peak = torch.cuda.max_memory_allocated() / 1e6  # MB\n",
        "    return peak\n",
        "\n",
        "def benchmark_kernel(fwd, base_benchmark_kwargs):\n",
        "    \n",
        "    def bwd(x, provider):\n",
        "        x.requires_grad = True\n",
        "        out = fwd(x, provider)\n",
        "        loss = out.sum()\n",
        "        loss.backward()\n",
        "\n",
        "\n",
        "    MAP_FWD_BKW = {\n",
        "        \"fwd\" : fwd,\n",
        "        \"bwd\" : bwd,\n",
        "    }\n",
        "    \n",
        "    configs = []\n",
        "    for bench_kind in ['timing', 'memory']:\n",
        "        for mode in ['fwd', 'bwd']:\n",
        "            _kwargs = deepcopy(base_benchmark_kwargs)\n",
        "            _kwargs['args'].update({\"mode\" : mode})\n",
        "            _kwargs['args'].update({\"bench_kind\" : bench_kind})\n",
        "            _kwargs['ylabel'] = 'ms' if bench_kind == 'timing' else 'MB'\n",
        "            _kwargs['plot_name'] += f' - {bench_kind} - {mode}'\n",
        "            configs.append(triton.testing.Benchmark(**_kwargs))\n",
        "\n",
        "    @triton.testing.perf_report(configs)\n",
        "    def benchmark(M, N, provider, mode, bench_kind):\n",
        "        x = torch.randn(M, N, device=DEVICE, dtype=torch.bfloat16)\n",
        "        stream = getattr(torch, DEVICE.type).Stream()\n",
        "        getattr(torch, DEVICE.type).set_stream(stream)\n",
        "        if bench_kind == \"timing\":\n",
        "            ms = triton.testing.do_bench(lambda: MAP_FWD_BKW[mode](x, provider))\n",
        "            # gbps = lambda ms: 2 * x.numel() * x.element_size() * 1e-9 / (ms * 1e-3)\n",
        "            return ms #gbps(ms)\n",
        "        elif bench_kind == \"memory\":\n",
        "            mem_mb = measure_memory(MAP_FWD_BKW[mode], x, provider)\n",
        "            return mem_mb\n",
        "        raise ValueError(f\"bench_kind must be either 'timing' or 'memory', got {bench_kind}\")\n",
        "\n",
        "    result_dfs = benchmark.run(show_plots=False, print_data=False, return_df=True)\n",
        "    for df, config in zip(result_dfs, configs):\n",
        "        df.plot(x='N', ylabel = config.ylabel, title=config.plot_name, legend=True)\n",
        "\n",
        "    return result_dfs\n",
        "\n",
        "benchmark_kernel(fwd, base_benchmark_kwargs)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
